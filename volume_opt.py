import os
import glob
import math
import gc
import joblib
import numpy as np
import pandas as pd
from datetime import datetime, timedelta

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import MiniBatchKMeans  # KMeansë³´ë‹¤ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ë¹ ë¦„
from collections import Counter

# --- 1. ë°ì´í„° ë¡œë” (ì‹ ê·œ í•¨ìˆ˜) ---
# ìš”ì²­í•˜ì‹  ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
def load_data_from_dumps(base_dir, ticker, interval, start_date_str, end_date_str, 
                         features_to_use):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ë‚ ì§œ ë²”ìœ„ ë‚´ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë³‘í•©í•©ë‹ˆë‹¤.
    """
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘... ({start_date_str} ~ {end_date_str})")
    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')
    
    all_dfs = []
    current_date = start_date
    
    while current_date <= end_date:
        date_str = current_date.strftime('%Y-%m-%d')
        # ./dumps/BTC/3m/2025-06-17/*.csv
        search_path = os.path.join(base_dir, ticker, interval, date_str, '*.csv')
        csv_files = sorted(glob.glob(search_path)) # 00.csv, 01.csv... ìˆœì„œ ë³´ì¥
        
        for f in csv_files:
            try:
                # ì²« ë²ˆì§¸ ì—´ì„ ì¸ë±ìŠ¤(ì‹œê°„)ë¡œ ì‚¬ìš©
                df = pd.read_csv(f, index_col=0, parse_dates=True)
                all_dfs.append(df)
            except Exception as e:
                print(f"ê²½ê³ : {f} íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        
        current_date += timedelta(days=1)
        
    if not all_dfs:
        print("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
        
    # ëª¨ë“  DataFrame ë³‘í•©
    full_df = pd.concat(all_dfs)
    full_df = full_df.sort_index() # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
    full_df = full_df[~full_df.index.duplicated(keep='first')] # ì¤‘ë³µ ì¸ë±ìŠ¤ ì œê±°
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    try:
        selected_df = full_df[features_to_use]
        selected_df = selected_df.dropna() # ì´ë™í‰ê·  ë“±ìœ¼ë¡œ ì¸í•œ NaN ê°’ ì œê±°
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ {len(selected_df)}ê°œì˜ í‹±(row) í™•ë³´.")
        return selected_df
    except KeyError as e:
        print(f"ì˜¤ë¥˜: ìš”ì²­ëœ í”¼ì²˜(ì»¬ëŸ¼) {e}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {full_df.columns.tolist()}")
        return pd.DataFrame()

# --- 2. PyTorch ë°ì´í„°ì…‹ (Priceìš©) ---
# ë‹¤ì¤‘ í”¼ì²˜(N, 8)ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì •
class PricePatternDataset(Dataset):
    """
    ë‹¤ì¤‘ í”¼ì²˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ (seq_len, num_features) í…ì„œë¡œ ë°˜í™˜í•˜ëŠ” ë°ì´í„°ì…‹
    """
    def __init__(self, data, seq_len):
        # dataëŠ” (N, num_features) í˜•íƒœì˜ ìŠ¤ì¼€ì¼ë§ëœ NumPy ë°°ì—´
        self.data = data
        self.seq_len = seq_len
        self.num_features = data.shape[1]
        
        if len(data) < seq_len:
            raise ValueError(f"ë°ì´í„° ê¸¸ì´({len(data)})ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´({seq_len})ë³´ë‹¤ ì§§ìŠµë‹ˆë‹¤.")

    def __len__(self):
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹
        return len(self.data) - self.seq_len + 1

    def __getitem__(self, idx):
        # (seq_len, num_features) í˜•íƒœì˜ ìƒ˜í”Œ ì¶”ì¶œ
        sample = self.data[idx : idx + self.seq_len]
        return torch.tensor(sample, dtype=torch.float32)

# --- 3. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ (ì‹ ê·œ ì •ì˜) ---
# ì˜ˆì‹œ ì½”ë“œì— ì—†ì—ˆë˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

class PositionalEncoding(nn.Module):
    """ Transformerë¥¼ ìœ„í•œ ìœ„ì¹˜ ì¸ì½”ë”© """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0) # [1, max_len, d_model] (batch_first=True ìš©)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: [batch, seq_len, d_model]
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class TransformerAutoencoder(nn.Module):
    """
    íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”-ë””ì½”ë” ê¸°ë°˜ ì˜¤í† ì¸ì½”ë” (íŒ¨í„´ ì••ì¶•ê¸°)
    [batch, seq_len, input_dim] -> [batch, latent_dim] -> [batch, seq_len, input_dim]
    """
    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, latent_dim, max_seq_len):
        super(TransformerAutoencoder, self).__init__()
        self.max_seq_len = max_seq_len
        self.d_model = d_model
        
        # 1. Input Embedding (input_dim -> d_model)
        self.input_embed = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)
        
        # 2. Encoder
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
        
        # 3. Bottleneck (d_model * seq_len -> latent_dim)
        # ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ flattení•˜ì—¬ ì••ì¶•
        self.to_latent = nn.Sequential(
            nn.Linear(d_model * max_seq_len, d_model),
            nn.ReLU(),
            nn.Linear(d_model, latent_dim)
        )
        
        # 4. Decoder Input (latent_dim -> d_model * seq_len)
        self.from_latent = nn.Sequential(
            nn.Linear(latent_dim, d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model * max_seq_len)
        )
        
        # 5. Decoder (TransformerEncoder ì¸µì„ ë””ì½”ë”ë¡œ í™œìš©)
        # num_decoder_layers íŒŒë¼ë¯¸í„° ì‚¬ìš©
        decoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_model*4, batch_first=True)
        self.transformer_decoder = nn.TransformerEncoder(decoder_layer, num_layers=num_decoder_layers)
        
        # 6. Output Layer (d_model -> input_dim)
        self.output_layer = nn.Linear(d_model, input_dim)

    def encode(self, src):
        # src: [batch, seq_len, input_dim]
        src_embed = self.input_embed(src) * math.sqrt(self.d_model)
        src_embed = self.pos_encoder(src_embed) # [batch, seq_len, d_model]
        
        enc_output = self.transformer_encoder(src_embed) # [batch, seq_len, d_model]
        
        # Flatten and project to latent
        enc_flat = enc_output.view(enc_output.size(0), -1) # [batch, seq_len * d_model]
        latent = self.to_latent(enc_flat) # [batch, latent_dim]
        return latent

    def decode(self, latent):
        # latent: [batch, latent_dim]
        dec_input_flat = self.from_latent(latent) # [batch, seq_len * d_model]
        dec_input = dec_input_flat.view(latent.size(0), self.max_seq_len, self.d_model)
        
        dec_input = self.pos_encoder(dec_input) # Add position
        
        dec_output = self.transformer_decoder(dec_input) # [batch, seq_len, d_model]
        
        output = self.output_layer(dec_output) # [batch, seq_len, input_dim]
        return output

    def forward(self, src):
        latent = self.encode(src)
        reconstructed = self.decode(latent)
        # ì¬êµ¬ì¶•ëœ ê°’ê³¼ ì ì¬ ë²¡í„°(íŒ¨í„´) ë™ì‹œ ë°˜í™˜
        return reconstructed, latent

# --- 4. ì¡°ê¸° ì¢…ë£Œ (Early Stopping) (ì‹ ê·œ í´ë˜ìŠ¤) ---
# [Req 6, 8] ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ì¡°ê¸° ì¢…ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth'):
        """
        Args:
            patience (int): Validation lossê°€ ê°œì„ ë˜ì§€ ì•Šì•„ë„ ê¸°ë‹¤ë¦´ epoch ìˆ˜
            verbose (bool): ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
            delta (float): ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰
            path (str): ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.inf
        self.delta = delta
        self.path = path

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'  [EarlyStopping] Counter: {self.counter} / {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        # [Req 8] ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
        if self.verbose:
            print(f'  [EarlyStopping] Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model to {self.path}')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss

# --- 5. ëª¨ë¸ í•™ìŠµ (ì‹ ê·œ í•¨ìˆ˜) ---
# [Req 6, 7] ì¡°ê¸° ì¢…ë£Œë¥¼ í¬í•¨í•œ í•™ìŠµ ë£¨í”„
def train_autoencoder(model, train_loader, val_loader, model_path, epochs, lr, patience, device):
    """
    íŠ¸ëœìŠ¤í¬ë¨¸ ì˜¤í† ì¸ì½”ë” í•™ìŠµ í•¨ìˆ˜
    """
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss() # ì¬êµ¬ì¶• ì˜¤ì°¨(Reconstruction Loss)
    
    # [Req 6, 8] ì¡°ê¸° ì¢…ë£Œ í•¸ë“¤ëŸ¬ (ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ model_pathì— ì €ì¥)
    early_stopper = EarlyStopping(patience=patience, verbose=True, path=model_path)
    
    print("\n" + "="*40)
    print("      ğŸš€  ì˜¤í† ì¸ì½”ë” í•™ìŠµ ì‹œì‘  ğŸš€")
    print("="*40)
    
    for epoch in range(1, epochs + 1):
        # --- Training ---
        model.train()
        train_loss = 0.0
        for data in train_loader:
            data = data.to(device) # (batch, seq_len, num_features)
            
            optimizer.zero_grad()
            reconstructed, _ = model(data)
            loss = criterion(reconstructed, data) # ì›ë³¸(data)ê³¼ ì¬êµ¬ì¶•(reconstructed) ë¹„êµ
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * data.size(0)
        
        # --- Validation ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for data in val_loader:
                data = data.to(device)
                reconstructed, _ = model(data)
                loss = criterion(reconstructed, data)
                val_loss += loss.item() * data.size(0)
        
        train_loss = train_loss / len(train_loader.dataset)
        val_loss = val_loss / len(val_loader.dataset)
        
        print(f'Epoch: {epoch:04d} \tTraining Loss: {train_loss:.6f} \tValidation Loss: {val_loss:.6f}')
        
        # --- Early Stopping Check ---
        early_stopper(val_loss, model)
        if early_stopper.early_stop:
            print("="*40)
            print("           â›” ì¡°ê¸° ì¢…ë£Œ â›”")
            print(f"ìµœì  Epoch: {epoch - early_stopper.patience}")
            print(f"ìµœì € Val Loss: {early_stopper.val_loss_min:.6f}")
            print("="*40)
            break
    
    # [Req 8] í•™ìŠµ ì™„ë£Œ í›„, ì €ì¥ëœ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ë¡œë“œ
    print(f"\ní•™ìŠµ ì¢…ë£Œ. '{model_path}'ì—ì„œ ìµœì  ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
    except Exception as e:
        print(f"ì˜¤ë¥˜: ìµœì  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ë§ˆì§€ë§‰ epoch ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        
    return model

# --- 6. ì¹´í…Œê³ ë¦¬ ìƒì„± (KMeans) (ì‹ ê·œ í•¨ìˆ˜) ---
# [Req 4, 5, 9]
def create_categories(model, dataloader, n_categories, device):
    """
    í•™ìŠµëœ Autoencoderë¥¼ ì´ìš©í•´ Latent Vectorë¥¼ ì¶”ì¶œí•˜ê³  KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    """
    model.to(device)
    model.eval()
    all_latents = []
    
    print("\nKMeans í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ ì ì¬ ë²¡í„°(Latent Vector) ì¶”ì¶œ ì¤‘...")
    with torch.no_grad():
        for data in dataloader:
            data = data.to(device)
            _, latent = model(data) # (reconstructed, latent)
            all_latents.append(latent.cpu())
    
    all_latents_np = torch.cat(all_latents, dim=0).numpy()
    print(f"ì´ {all_latents_np.shape[0]}ê°œì˜ ì ì¬ ë²¡í„° ì¶”ì¶œ ì™„ë£Œ. (í˜•íƒœ: {all_latents_np.shape})")
    
    print(f"MiniBatchKMeans í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘ (N={n_categories})...")
    # MiniBatchKMeansëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ KMeansë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„
    kmeans = MiniBatchKMeans(n_clusters=n_categories, 
                            random_state=42, 
                            n_init=10, 
                            batch_size=min(1024, len(all_latents_np)))
    kmeans.fit(all_latents_np)
    print("KMeans í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ.")
    
    # --- [Req 9] ì¹´í…Œê³ ë¦¬ í¸ì¤‘ë„(ë¶„í¬) ì¶œë ¥ ---
    labels = kmeans.labels_
    label_counts = Counter(labels)
    sorted_counts = label_counts.most_common() # (label, count) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
    
    print("\n" + "="*40)
    print("    ğŸ“Š Top 5 Most Frequent Categories ğŸ“Š")
    print("="*40)
    total_samples = len(labels)
    for i, (label, count) in enumerate(sorted_counts[:30]):
        percentage = (count / total_samples) * 100
        print(f"  {i+1}. Category {label:03d}: {count}ê°œ ìƒ˜í”Œ ({percentage:.2f}%)")
    print("="*40 + "\n")
    
    return kmeans

# --- 7. ì¹´í…Œê³ ë¦¬ ì¶”ë¡  (ì‹ ê·œ í•¨ìˆ˜) ---
# [Req 5] ì €ì¥ëœ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/KMeansë¡œ ìƒˆ ë°ì´í„°ì˜ íŒ¨í„´ ID ì¶”ë¡ 
def get_pattern_category(new_data_ticks, autoencoder, kmeans_model, scaler, seq_len, device):
    """
    ìƒˆë¡œìš´ ë°ì´í„°(Aí‹±)ë¥¼ ë°›ì•„ ì–´ë–¤ íŒ¨í„´ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ ì¶”ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        new_data_ticks (np.array): (seq_len, num_features) í˜•íƒœì˜ ì›ë³¸ ë°ì´í„°
    """
    
    # ì…ë ¥ ë°ì´í„° ê²€ì¦
    if new_data_ticks.shape[0] != seq_len:
        raise ValueError(f"ì…ë ¥ ë°ì´í„° ê¸¸ì´ëŠ” {seq_len}ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (í˜„ì¬: {new_data_ticks.shape[0]})")
    if new_data_ticks.shape[1] != scaler.n_features_in_:
        raise ValueError(f"ì…ë ¥ í”¼ì²˜ ê°œìˆ˜ëŠ” {scaler.n_features_in_}ê°œì—¬ì•¼ í•©ë‹ˆë‹¤. (í˜„ì¬: {new_data_ticks.shape[1]})")

    # 1. Scale
    scaled_data = scaler.transform(new_data_ticks)
    
    # 2. Convert to Tensor (Batch ì°¨ì› ì¶”ê°€)
    data_tensor = torch.tensor(scaled_data, dtype=torch.float32).unsqueeze(0).to(device)
    
    # 3. Get latent vector
    autoencoder.to(device)
    autoencoder.eval()
    with torch.no_grad():
        _, latent = autoencoder(data_tensor) # (reconstructed, latent)
    
    # 4. Predict category
    latent_np = latent.cpu().numpy()
    category = kmeans_model.predict(latent_np)
    
    return category[0] # [batch_size=1]ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜

# --- 8. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == '__main__':

    for ___ in range(100):
    
        # --- [Req 7] CUDA/CPU ì¥ì¹˜ ê²€ì¦ ---
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("="*40)
        print("       ì¥ì¹˜ ê²€ì¦ (Device Verification)       ")
        print("="*40)
        print(f"PyTorch ë²„ì „: {torch.__version__}")
        print(f"CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ì‚¬ìš© ì¤‘ì¸ GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("!! ê²½ê³ : CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        print(f"ì„ íƒëœ ì¥ì¹˜: {device}")
        print("="*40)

        # --- 0. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
        BASE_DIR = './dumps'
        TICKER = 'BTC'
        INTERVAL = '3m'
        START_DATE = '2025-06-17'
        END_DATE = '2025-11-05'
        
        # [Req 1] ì‚¬ìš©í•  í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ (CSV ì»¬ëŸ¼ëª…ê³¼ ì¼ì¹˜í•´ì•¼ í•¨)
        FEATURES = ['open', 'high', 'low', 'close', 'volume', 'ma5', 'ma7', 'ma10']
        INPUT_DIM = len(FEATURES) # 8
        
        # [Req 1, 3] ê³¼ê±° "A"í‹± (ìƒ˜í”Œ ê¸¸ì´). 30~300 ì‚¬ì´ë¡œ ì„¤ì •.
        SEQUENCE_LENGTH = 50
        MAX_SEQ_LEN = SEQUENCE_LENGTH # Positional Encodingì„ ìœ„í•´ ëª¨ë¸ì— ì „ë‹¬
        
        # ëª¨ë¸ íŒŒë¼ë¯¸í„°
        D_MODEL = 64
        NHEAD = 4
        NUM_ENCODER_LAYERS = 3
        NUM_DECODER_LAYERS = 3
        LATENT_DIM = 32          # 32ì°¨ì›ìœ¼ë¡œ íŒ¨í„´ ì••ì¶•
        
        # [Req 4] ì¹´í…Œê³ ë¦¬ ìˆ˜
        N_CATEGORIES = 500
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        BATCH_SIZE = 4096*2
        EPOCHS = 500            # [Req 6] ì¡°ê¸° ì¢…ë£Œë˜ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
        VALIDATION_SPLIT_RATIO = 0.1 # 10%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
        
        # [Req 6] ì¡°ê¸° ì¢…ë£Œ Patience
        EARLY_STOPPING_PATIENCE = 50
        LEARNING_RATE = 1e-4
        
        # [Req 10] Train Type (ê²½ë¡œëª…ì— ì‚¬ìš©)
        TRAIN_TYPE = 'price' # 'volume' -> 'price'ë¡œ ë³€ê²½
        
        # ì €ì¥ íŒŒì¼ëª…
        MODELS_DIR = './models'
        if not os.path.exists(MODELS_DIR): os.makedirs(MODELS_DIR)
        ticker_dir = os.path.join(MODELS_DIR, TICKER)
        if not os.path.exists(ticker_dir): os.makedirs(ticker_dir)
        interval_dir = os.path.join(ticker_dir, INTERVAL)
        if not os.path.exists(interval_dir): os.makedirs(interval_dir)
        train_type_dir = os.path.join(interval_dir, TRAIN_TYPE)
        if not os.path.exists(train_type_dir): os.makedirs(train_type_dir)
        sequence_length_dir = os.path.join(train_type_dir, str(SEQUENCE_LENGTH))
        if not os.path.exists(sequence_length_dir): os.makedirs(sequence_length_dir)
        
        # [Req 2, 5, 8] ì €ì¥ ê²½ë¡œ
        SCALER_PATH = os.path.join(sequence_length_dir, f'{TRAIN_TYPE}_scaler.joblib')
        MODEL_PATH = os.path.join(sequence_length_dir, 'transformer_autoencoder.pth') # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        KMEANS_PATH = os.path.join(sequence_length_dir, 'pattern_categories.joblib')

        # --- 1. ë°ì´í„° ë¡œë“œ ---
        full_df = load_data_from_dumps(BASE_DIR, TICKER, INTERVAL, START_DATE, END_DATE, FEATURES)
        
        if full_df.empty:
            print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            # (í•„ìš”ì‹œ ê°€ìƒ ë°ì´í„° ìƒì„±)
            # print("ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (10000 í‹±).")
            # data_dict = {f: np.random.rand(10000) for f in FEATURES}
            # full_df = pd.DataFrame(data_dict)
        else:
            print(f"ì´ {len(full_df)}ê°œì˜ í‹±(row)ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # --- 2. ì „ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ë§) [Req 2] ---
        # ScalerëŠ” ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ **í•™ìŠµ ë°ì´í„°(train_data) ê¸°ì¤€**ìœ¼ë¡œ fití•´ì•¼
        # Data Leakage(ë°ì´í„° ìœ ì¶œ)ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì‹œê³„ì—´ì´ë¯€ë¡œ shuffle=Falseë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        
        # (1) Train/Validation ë°ì´í„° ë¶„ë¦¬ (DataFrame ê¸°ì¤€)
        train_df, val_df = train_test_split(full_df, 
                                            test_size=VALIDATION_SPLIT_RATIO, 
                                            shuffle=False) # ì‹œê³„ì—´ì´ë¯€ë¡œ ìˆœì„œ ìœ ì§€
        
        # (2) Scaler ë¡œë“œ ë˜ëŠ” ìƒì„±
        if os.path.exists(SCALER_PATH):
            print(f"\n'{SCALER_PATH}'ì—ì„œ ê¸°ì¡´ Scalerë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
            scaler = joblib.load(SCALER_PATH)
        else:
            print(f"\n'{SCALER_PATH}'ì— Scalerê°€ ì—†ìŠµë‹ˆë‹¤. Train ë°ì´í„°ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            scaler = StandardScaler()
            # Train ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œë§Œ fit
            scaler.fit(train_df[FEATURES])
            joblib.dump(scaler, SCALER_PATH)
            print(f"Scaler ì €ì¥ ì™„ë£Œ: {SCALER_PATH}")

        # (3.1) ì „ì²´ ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§ (KMeans í•™ìŠµìš©)
        scaled_data_full = scaler.transform(full_df[FEATURES])
        # (3.2) Train/Val ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§ (ëª¨ë¸ í•™ìŠµìš©)
        scaled_data_train = scaler.transform(train_df[FEATURES])
        scaled_data_val = scaler.transform(val_df[FEATURES])

        print(f"ë°ì´í„° ë¶„í• : Train {len(scaled_data_train)}ê°œ, Validation {len(scaled_data_val)}ê°œ")

        # --- 3. Dataset ë° DataLoader (Train/Validation ë¶„ë¦¬) ---
        # [Req 6]
        train_dataset = PricePatternDataset(scaled_data_train, SEQUENCE_LENGTH)
        val_dataset = PricePatternDataset(scaled_data_val, SEQUENCE_LENGTH)

        # í•™ìŠµ ë°ì´í„°ëŠ” ì„ì–´ì„œ(shuffle=True) ëª¨ë¸ì´ ìˆœì„œì— ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€
        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        # ê²€ì¦ ë°ì´í„°ëŠ” ìˆœì„œëŒ€ë¡œ(shuffle=False) í‰ê°€
        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

        # (ë°ì´í„° í˜•íƒœ í™•ì¸)
        try:
            sample_data = next(iter(train_dataloader))
            print(f"ë°ì´í„°ì…‹ ìƒ˜í”Œ í˜•íƒœ (Batch, Seq_Len, Features): {sample_data.shape}")
        except ValueError as e:
            print(f"ë°ì´í„°ì…‹ ìƒì„± ì˜¤ë¥˜: {e}")
            print("ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ 1ê°œì˜ ë°°ì¹˜ë„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. START_DATEë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            exit()


        # --- 4. Autoencoder ëª¨ë¸ ì´ˆê¸°í™” ---
        autoencoder_model = TransformerAutoencoder(
            input_dim=INPUT_DIM, 
            d_model=D_MODEL, 
            nhead=NHEAD,
            num_encoder_layers=NUM_ENCODER_LAYERS,
            num_decoder_layers=NUM_DECODER_LAYERS,
            latent_dim=LATENT_DIM, 
            max_seq_len=MAX_SEQ_LEN
        )

        # --- [Req 7, 8] ëª¨ë¸ ë¡œë“œ (í•™ìŠµ ì¬ê°œ) ---
        # EarlyStoppingì´ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ MODEL_PATHì— ì €ì¥í•˜ë¯€ë¡œ,
        # ì´ ê²½ë¡œëŠ” 'ìµœê·¼' ëª¨ë¸ì´ ì•„ë‹Œ 'ìµœì ' ëª¨ë¸ì…ë‹ˆë‹¤.
        if os.path.exists(MODEL_PATH):
            print(f"\n'{MODEL_PATH}' ì—ì„œ ê¸°ì¡´ ë² ìŠ¤íŠ¸ ëª¨ë¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            try:
                autoencoder_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
                print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ. ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.\n")
            except Exception as e:
                print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")
        else:
            print(f"\n'{MODEL_PATH}' ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")

        # --- 5. Autoencoder ëª¨ë¸ í•™ìŠµ (ì¡°ê¸° ì¢…ë£Œ ì ìš©) ---
        # [Req 6]
        autoencoder_model = train_autoencoder(
            model=autoencoder_model, 
            train_loader=train_dataloader, 
            val_loader=val_dataloader, 
            model_path=MODEL_PATH,  # ë² ìŠ¤íŠ¸ ëª¨ë¸ì´ ì—¬ê¸°ì— ì €ì¥ë¨
            epochs=EPOCHS, 
            lr=LEARNING_RATE, 
            patience=EARLY_STOPPING_PATIENCE,
            device=device
        )
        
        # --- 6. "ì¹´í…Œê³ ë¦¬" ìƒì„± (KMeans) ---
        # [Req 4, 5, 9]
        # train_autoencoder í•¨ìˆ˜ê°€ ìµœì  ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜í–ˆìœ¼ë¯€ë¡œ
        # autoencoder_modelì€ í˜„ì¬ 'ë² ìŠ¤íŠ¸ ëª¨ë¸' ìƒíƒœì…ë‹ˆë‹¤.
        
        # K-MeansëŠ” ì „ì²´ ë°ì´í„°(Train+Val)ë¡œ ìƒì„±
        full_dataset = PricePatternDataset(scaled_data_full, SEQUENCE_LENGTH)
        # K-Means í•™ìŠµ ì‹œì—ëŠ” ë°ì´í„°ë¥¼ ì„ì„ í•„ìš” ì—†ìŒ
        full_dataloader = DataLoader(full_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        kmeans_model = create_categories(
            model=autoencoder_model, 
            dataloader=full_dataloader, 
            n_categories=N_CATEGORIES,
            device=device
        )
        
        # [Req 5] KMeans ëª¨ë¸ ì €ì¥
        joblib.dump(kmeans_model, KMEANS_PATH)
        print(f"KMeans (ì¹´í…Œê³ ë¦¬) ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {KMEANS_PATH}")

        
        # --- 7. "ì¹´í…Œê³ ë¦¬" ì¬ì‚¬ìš© ì˜ˆì‹œ ---
        print("\n" + "="*40)
        print("      ğŸ”„ ì¹´í…Œê³ ë¦¬ ì¬ì‚¬ìš©(ì¶”ë¡ ) í…ŒìŠ¤íŠ¸ ğŸ”„")
        print("="*40)
        
        # (ê°€ìƒì˜ ìƒˆ ë°ì´í„° 100í‹±, 8í”¼ì²˜)
        # [Req 1] (SEQUENCE_LENGTH, INPUT_DIM) í˜•íƒœì˜ NumPy ë°°ì—´
        new_ticks_A = np.random.rand(SEQUENCE_LENGTH, INPUT_DIM) * 100 + 150000000
        
        # ì €ì¥ëœ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        print("ì €ì¥ëœ ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬/KMeans ë¡œë“œ ì¤‘...")
        loaded_autoencoder = TransformerAutoencoder(
            input_dim=INPUT_DIM, d_model=D_MODEL, nhead=NHEAD,
            num_encoder_layers=NUM_ENCODER_LAYERS,
            num_decoder_layers=NUM_DECODER_LAYERS,
            latent_dim=LATENT_DIM, max_seq_len=MAX_SEQ_LEN
        )
        loaded_autoencoder.load_state_dict(torch.load(MODEL_PATH, map_location=device))
        
        loaded_scaler = joblib.load(SCALER_PATH)
        loaded_kmeans = joblib.load(KMEANS_PATH)

        print("ì¶”ë¡  ì‹œì‘...")
        category = get_pattern_category(
            new_data_ticks=new_ticks_A, 
            autoencoder=loaded_autoencoder, 
            kmeans_model=loaded_kmeans, 
            scaler=loaded_scaler, 
            seq_len=SEQUENCE_LENGTH,
            device=device
        )
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ìƒˆë¡œìš´ {SEQUENCE_LENGTH}í‹± ë°ì´í„°ì˜ íŒ¨í„´ ì¹´í…Œê³ ë¦¬: {category}")
        print("="*40)
        
        # --- ë©”ëª¨ë¦¬ ì •ë¦¬ (ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ì´ˆê¸°í™”) ---
        print("\n[ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...]")
        
        # 1. ëª¨ë¸ë“¤ì„ CPUë¡œ ì´ë™ í›„ ì‚­ì œ
        if 'autoencoder_model' in locals():
            autoencoder_model.cpu()
            del autoencoder_model
        
        if 'loaded_autoencoder' in locals():
            loaded_autoencoder.cpu()
            del loaded_autoencoder
        
        # 2. ë°ì´í„°ë¡œë”ì™€ ë°ì´í„°ì…‹ ì‚­ì œ
        if 'train_dataloader' in locals():
            del train_dataloader
        if 'val_dataloader' in locals():
            del val_dataloader
        if 'full_dataloader' in locals():
            del full_dataloader
        if 'train_dataset' in locals():
            del train_dataset
        if 'val_dataset' in locals():
            del val_dataset
        if 'full_dataset' in locals():
            del full_dataset
        
        # 3. ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ë°°ì—´ ì‚­ì œ
        if 'scaled_data_full' in locals():
            del scaled_data_full
        if 'scaled_data_train' in locals():
            del scaled_data_train
        if 'scaled_data_val' in locals():
            del scaled_data_val
        
        # 4. DataFrame ì‚­ì œ
        if 'full_df' in locals():
            del full_df
        if 'train_df' in locals():
            del train_df
        if 'val_df' in locals():
            del val_df
        
        # 5. ê¸°íƒ€ ë³€ìˆ˜ ì‚­ì œ
        if 'scaler' in locals():
            del scaler
        if 'loaded_scaler' in locals():
            del loaded_scaler
        if 'kmeans_model' in locals():
            del kmeans_model
        if 'loaded_kmeans' in locals():
            del loaded_kmeans
        if 'new_ticks_A' in locals():
            del new_ticks_A
        if 'sample_data' in locals():
            del sample_data
        
        # 6. CUDA ìºì‹œ ì •ë¦¬ (GPU ë©”ëª¨ë¦¬ í•´ì œ)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print(f"  âœ“ GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # 7. Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        print(f"  âœ“ Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ")
        
        print("[ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ]\n")